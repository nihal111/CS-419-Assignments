Innovations:

1. Tree bagging (Random Forest): Multiple trees are built with 70% of the data randomly 
sampled from the training data. Performaceof each tree is calculated on the hold out set.
This gives us a indicative measure of how accurate the tree's classification is. 
Multiple trees are bagged during training. Each test sample is passed through all of 
the trees and the majority vote is taken as the final answer for the price of the test sample.

2. Pruning: Remove nodes from the fully built tree by checking if it helps improve the cross 
validation accuracy. Run multiple iterations till improve in accuracy is not significant.

Combine 1 and 2 to produce an effective solution.